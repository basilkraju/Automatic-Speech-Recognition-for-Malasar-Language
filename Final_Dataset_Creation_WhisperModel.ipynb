{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This program aims to create a datasect with a features audio and sentence\n",
    " and it will upload the dataset into huggingface.\n",
    " But this program will not create a training and test dataset.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"If we need to create train and test split we need to create an object\n",
    "for the DatasetDict and assign each seperate split into two different list.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved to C:\\Users\\basil\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['luk0060035.wav', 'luke00101.wav', 'luke001010.wav', 'luke001011.wav', 'luke001012.wav', 'luke001013.wav', 'luke001014.wav', 'luke001015.wav', 'luke001016.wav', 'luke001017.wav', 'luke001018.wav', 'luke001019.wav', 'luke00102.wav', 'luke001020.wav', 'luke001021.wav', 'luke001022.wav', 'luke001023.wav', 'luke001024.wav', 'luke001025.wav', 'luke001026.wav', 'luke001027.wav', 'luke001028.wav', 'luke001029.wav', 'luke00103.wav', 'luke001030.wav', 'luke001031.wav', 'luke001032.wav', 'luke001033.wav', 'luke001034.wav', 'luke001035.wav', 'luke001036.wav', 'luke001037.wav', 'luke001038.wav', 'luke001039.wav', 'luke00104.wav', 'luke001040.wav', 'luke001041.wav', 'luke001042.wav', 'luke00105.wav', 'luke00106.wav', 'luke00107.wav', 'luke00108.wav', 'luke00109.wav', 'luke00111.wav', 'luke001110.wav', 'luke001111.wav', 'luke001112.wav', 'luke001113.wav', 'luke001114.wav', 'luke001115.wav', 'luke001116.wav', 'luke001117.wav', 'luke001118.wav', 'luke001119.wav', 'luke00112.wav', 'luke001120.wav', 'luke001121.wav', 'luke001122.wav', 'luke001123.wav', 'luke001124.wav', 'luke001125.wav', 'luke001126.wav', 'luke001127.wav', 'luke001128.wav', 'luke001129.wav', 'luke00113.wav', 'luke001130.wav', 'luke001131.wav', 'luke001132.wav', 'luke001133.wav', 'luke001134.wav', 'luke001135.wav', 'luke001136.wav', 'luke001137.wav', 'luke001138.wav', 'luke001139.wav', 'luke00114.wav', 'luke001140.wav', 'luke001141.wav', 'luke001142.wav', 'luke001143.wav', 'luke001144.wav', 'luke001145.wav', 'luke001146.wav', 'luke001147.wav', 'luke001148.wav', 'luke001149.wav', 'luke00115.wav', 'luke001150.wav', 'luke001151.wav', 'luke001152.wav', 'luke001153.wav', 'luke00116.wav', 'luke00117.wav', 'luke00118.wav', 'luke00119.wav', 'luke00121.wav', 'luke001210.wav', 'luke001211.wav', 'luke001212.wav', 'luke001213.wav', 'luke001214.wav', 'luke001215.wav', 'luke001216.wav', 'luke001217.wav', 'luke001218.wav', 'luke001219.wav', 'luke00122.wav', 'luke001220.wav', 'luke001221.wav', 'luke001222.wav', 'luke001223.wav', 'luke001224.wav', 'luke001225.wav', 'luke001226.wav', 'luke001227.wav', 'luke001228.wav', 'luke001229.wav', 'luke00123.wav', 'luke001230.wav', 'luke001231.wav', 'luke001232.wav', 'luke001233.wav', 'luke001234.wav', 'luke001235.wav', 'luke001236.wav', 'luke001237.wav', 'luke001238.wav', 'luke001239.wav', 'luke00124.wav', 'luke001240.wav', 'luke001241.wav', 'luke001242.wav', 'luke001243.wav', 'luke001244.wav', 'luke001245.wav', 'luke001246.wav', 'luke001247.wav', 'luke001248.wav', 'luke001249.wav', 'luke00125.wav', 'luke001250.wav', 'luke001251.wav', 'luke001252.wav', 'luke001253.wav', 'luke001254.wav', 'luke001255.wav', 'luke001256.wav', 'luke001257.wav', 'luke001258.wav', 'luke001259.wav', 'luke00126.wav', 'luke00127.wav', 'luke00128.wav', 'luke00129.wav', 'luke00131.wav', 'luke001310.wav', 'luke001311.wav', 'luke001312.wav', 'luke001313.wav', 'luke001314.wav', 'luke001315.wav', 'luke001316.wav', 'luke001317.wav', 'luke001318.wav', 'luke001319.wav', 'luke00132.wav', 'luke001320.wav', 'luke001321.wav', 'luke001322.wav', 'luke001323.wav', 'luke001324.wav', 'luke001325.wav', 'luke001326.wav', 'luke001327.wav', 'luke001328.wav', 'luke001329.wav', 'luke00133.wav', 'luke001330.wav', 'luke001331.wav', 'luke001332.wav', 'luke001333.wav', 'luke001334.wav', 'luke001335.wav', 'luke00134.wav', 'luke00135.wav', 'luke00136.wav', 'luke00137.wav', 'luke00138.wav', 'luke00139.wav', 'luke00141.wav', 'luke001410.wav', 'luke001411.wav', 'luke001412.wav', 'luke001413.wav', 'luke001415.wav', 'luke001417.wav', 'luke001418.wav', 'luke001419.wav', 'luke00142.wav', 'luke001420.wav', 'luke001421.wav', 'luke001422.wav', 'luke001423.wav', 'luke001424.wav', 'luke001425.wav', 'luke001426.wav', 'luke001427.wav', 'luke001428.wav', 'luke001429.wav', 'luke00143.wav', 'luke001430.wav', 'luke001431.wav', 'luke001432.wav', 'luke001433.wav', 'luke001434.wav', 'luke001435.wav', 'luke00144.wav', 'luke00145.wav', 'luke00146.wav', 'luke00147.wav', 'luke00148.wav', 'luke00149.wav', 'luke00151.wav', 'luke001510.wav', 'luke001511.wav', 'luke001512.wav', 'luke001513.wav', 'luke001514.wav', 'luke001515.wav', 'luke001516.wav', 'luke001517.wav', 'luke001518.wav', 'luke001519.wav', 'luke00152.wav', 'luke001520.wav', 'luke001521.wav', 'luke001522.wav', 'luke001523.wav', 'luke001525.wav', 'luke001526.wav', 'luke001527.wav', 'luke001528.wav', 'luke001529.wav', 'luke00153.wav', 'luke001530.wav', 'luke001531.wav', 'luke001532.wav', 'luke001533.wav', 'luke00154.wav', 'luke00155.wav', 'luke00156.wav', 'luke00157.wav', 'luke00158.wav', 'luke00159.wav', 'luke00161.wav', 'luke001610.wav', 'luke001611.wav', 'luke001612.wav', 'luke001613.wav', 'luke001614.wav', 'luke001615.wav', 'luke001616.wav', 'luke001617.wav', 'luke001618.wav', 'luke001619.wav', 'luke00162.wav', 'luke001620.wav', 'luke001621.wav', 'luke001622.wav', 'luke001623.wav', 'luke001624.wav', 'luke001625.wav', 'luke001626.wav', 'luke001627.wav', 'luke001628.wav', 'luke001629.wav', 'luke00163.wav', 'luke001630.wav', 'luke001631.wav', 'luke00164.wav', 'luke00165.wav', 'luke00166.wav', 'luke00167.wav', 'luke00168.wav', 'luke00169.wav', 'luke00171.wav', 'luke001710.wav', 'luke001711.wav', 'luke001712.wav', 'luke001713.wav', 'luke001714.wav', 'luke001715.wav', 'luke001716.wav', 'luke001717.wav', 'luke001718.wav', 'luke001719.wav', 'luke00172.wav', 'luke001720.wav', 'luke001721.wav', 'luke001722.wav', 'luke001723.wav', 'luke001724.wav', 'luke001725.wav', 'luke001726.wav', 'luke001727.wav', 'luke001728.wav', 'luke001729.wav', 'luke00173.wav', 'luke001730.wav', 'luke001731.wav', 'luke001732.wav', 'luke001733.wav', 'luke001734.wav', 'luke001735.wav', 'luke001736.wav', 'luke001737.wav', 'luke00174.wav', 'luke00175.wav', 'luke00176.wav', 'luke00177.wav', 'luke00178.wav', 'luke00179.wav', 'luke00181.wav', 'luke001810.wav', 'luke001811.wav', 'luke001812.wav', 'luke001813.wav', 'luke001814.wav', 'luke001815.wav', 'luke001816.wav', 'luke001817.wav', 'luke001818.wav', 'luke001819.wav', 'luke00182.wav', 'luke001820.wav', 'luke001821.wav', 'luke001822.wav', 'luke001823.wav', 'luke001824.wav', 'luke001825.wav', 'luke001826.wav', 'luke001827.wav', 'luke001828.wav', 'luke001829.wav', 'luke00183.wav', 'luke001830.wav', 'luke001831.wav', 'luke001832.wav', 'luke001833.wav', 'luke001834.wav', 'luke001835.wav', 'luke001836.wav', 'luke001837.wav', 'luke001838.wav', 'luke001839.wav', 'luke00184.wav', 'luke001840.wav', 'luke001841.wav', 'luke001842.wav', 'luke001843.wav', 'luke00185.wav', 'luke00186.wav', 'luke00187.wav', 'luke00188.wav', 'luke00189.wav', 'luke00191.wav', 'luke001910.wav', 'luke001911.wav', 'luke001912.wav', 'luke001913.wav', 'luke001914.wav', 'luke001915.wav', 'luke001916.wav', 'luke001917.wav', 'luke001918.wav', 'luke001919.wav', 'luke00192.wav', 'luke001920.wav', 'luke001921.wav', 'luke001922.wav', 'luke001923.wav', 'luke001924.wav', 'luke001925.wav', 'luke001926.wav', 'luke001927.wav', 'luke001928.wav', 'luke001929.wav', 'luke00193.wav', 'luke001930.wav', 'luke001931.wav', 'luke001932.wav', 'luke001933.wav', 'luke001934.wav', 'luke001935.wav', 'luke001936.wav', 'luke001937.wav', 'luke001938.wav', 'luke001939.wav', 'luke00194.wav', 'luke001940.wav', 'luke001941.wav', 'luke001942.wav', 'luke001943.wav', 'luke001944.wav', 'luke001945.wav', 'luke001946.wav', 'luke001947.wav', 'luke001948.wav', 'luke00195.wav', 'luke00196.wav', 'luke00197.wav', 'luke00198.wav', 'luke00199.wav', 'luke002001.wav', 'luke0020010.wav', 'luke0020011.wav', 'luke0020012.wav', 'luke0020013.wav', 'luke0020014.wav', 'luke0020015.wav', 'luke0020016.wav', 'luke0020017.wav', 'luke0020018.wav', 'luke0020019.wav', 'luke002002.wav', 'luke0020020.wav', 'luke0020021.wav', 'luke0020022.wav', 'luke0020023.wav', 'luke0020024.wav', 'luke0020025.wav', 'luke0020026.wav', 'luke0020027.wav', 'luke0020028.wav', 'luke0020029.wav', 'luke002003.wav', 'luke0020030.wav', 'luke0020031.wav', 'luke0020032.wav', 'luke0020033.wav', 'luke0020034.wav', 'luke0020035.wav', 'luke0020036.wav', 'luke0020037.wav', 'luke0020038.wav', 'luke0020039.wav', 'luke002004.wav', 'luke0020040.wav', 'luke0020041.wav', 'luke0020042.wav', 'luke0020043.wav', 'luke0020044.wav', 'luke0020045.wav', 'luke0020046.wav', 'luke0020047.wav', 'luke002005.wav', 'luke002006.wav', 'luke002007.wav', 'luke002008.wav', 'luke002009.wav', 'luke00201.wav', 'luke002010.wav', 'luke002011.wav', 'luke002012.wav', 'luke002013.wav', 'luke002014.wav', 'luke002015.wav', 'luke002016.wav', 'luke002017.wav', 'luke002018.wav', 'luke002019.wav', 'luke00202.wav', 'luke002020.wav', 'luke002021.wav', 'luke002022.wav', 'luke002023.wav', 'luke002024.wav', 'luke002025.wav', 'luke002026.wav', 'luke002027.wav', 'luke002028.wav', 'luke002029.wav', 'luke00203.wav', 'luke002030.wav', 'luke002031.wav', 'luke002032.wav', 'luke002033.wav', 'luke002034.wav', 'luke002035.wav', 'luke002036.wav', 'luke002037.wav', 'luke002038.wav', 'luke002039.wav', 'luke00204.wav', 'luke002040.wav', 'luke002041.wav', 'luke002042.wav', 'luke002043.wav', 'luke002044.wav', 'luke002045.wav', 'luke002046.wav', 'luke002047.wav', 'luke002048.wav', 'luke002049.wav', 'luke00205.wav', 'luke002050.wav', 'luke002051.wav', 'luke002052.wav', 'luke00206.wav', 'luke00207.wav', 'luke00208.wav', 'luke00209.wav', 'luke00211.wav', 'luke002110.wav', 'luke002111.wav', 'luke002112.wav', 'luke002113.wav', 'luke002114.wav', 'luke002115.wav', 'luke002116.wav', 'luke002117.wav', 'luke002118.wav', 'luke002119.wav', 'luke00212.wav', 'luke002120.wav', 'luke002121.wav', 'luke002122.wav', 'luke002123.wav', 'luke002124.wav', 'luke002125.wav', 'luke002126.wav', 'luke002128.wav', 'luke002129.wav', 'luke00213.wav', 'luke002130.wav', 'luke002131.wav', 'luke002132.wav', 'luke002133.wav', 'luke002134.wav', 'luke002135.wav', 'luke002136.wav', 'luke002137.wav', 'luke002138.wav', 'luke002139.wav', 'luke00214.wav', 'luke00215.wav', 'luke00216.wav', 'luke00217.wav', 'luke00218.wav', 'luke00219.wav', 'luke00221.wav', 'luke002210.wav', 'luke002211.wav', 'luke002212.wav', 'luke002213.wav', 'luke002214.wav', 'luke002215.wav', 'luke002216.wav', 'luke002217.wav', 'luke002218.wav', 'luke002219.wav', 'luke00222.wav', 'luke002220.wav', 'luke002221.wav', 'luke002222.wav', 'luke002223.wav', 'luke002224.wav', 'luke002225.wav', 'luke002226.wav', 'luke002227.wav', 'luke002228.wav', 'luke002229.wav', 'luke00223.wav', 'luke002230.wav', 'luke002231.wav', 'luke002232.wav', 'luke002233.wav', 'luke002234.wav', 'luke002235.wav', 'luke002236.wav', 'luke002237.wav', 'luke002238.wav', 'luke002239.wav', 'luke00224.wav', 'luke002240.wav', 'luke002241.wav', 'luke002242.wav', 'luke002243.wav', 'luke002244.wav', 'luke002245.wav', 'luke002246.wav', 'luke002247.wav', 'luke002248.wav', 'luke002249.wav', 'luke00225.wav', 'luke002250.wav', 'luke002251.wav', 'luke002252.wav', 'luke002253.wav', 'luke002254.wav', 'luke002255.wav', 'luke002256.wav', 'luke002257.wav', 'luke002258.wav', 'luke002259.wav', 'luke00226.wav', 'luke002260.wav', 'luke002261.wav', 'luke002262.wav', 'luke002263.wav', 'luke002264.wav', 'luke002265.wav', 'luke002266.wav', 'luke002267.wav', 'luke002268.wav', 'luke002269.wav', 'luke00227.wav', 'luke002270.wav', 'luke002271.wav', 'luke00228.wav', 'luke00229.wav', 'luke00301.wav', 'luke003010.wav', 'luke003011.wav', 'luke003012.wav', 'luke003013.wav', 'luke003014.wav', 'luke003015.wav', 'luke003016.wav', 'luke003017.wav', 'luke003018.wav', 'luke003019.wav', 'luke00302.wav', 'luke003020.wav', 'luke003021.wav', 'luke003022.wav', 'luke003023.wav', 'luke003024.wav', 'luke003025.wav', 'luke003026.wav', 'luke003027.wav', 'luke003028.wav', 'luke003029.wav', 'luke00303.wav', 'luke003030.wav', 'luke003031.wav', 'luke003032.wav', 'luke003033.wav', 'luke003034.wav', 'luke003035.wav', 'luke003036.wav', 'luke003037.wav', 'luke003038.wav', 'luke00304.wav', 'luke00305.wav', 'luke00306.wav', 'luke00307.wav', 'luke00308.wav', 'luke00309.wav', 'luke00401.wav', 'luke004010.wav', 'luke004011.wav', 'luke004012.wav', 'luke004013.wav', 'luke004014.wav', 'luke004015.wav', 'luke004016.wav', 'luke004017.wav', 'luke004018.wav', 'luke004019.wav', 'luke00402.wav', 'luke004020.wav', 'luke004021.wav', 'luke004022.wav', 'luke004023.wav', 'luke004024.wav', 'luke004025.wav', 'luke004026.wav', 'luke004027.wav', 'luke004028.wav', 'luke004029.wav', 'luke00403.wav', 'luke004030.wav', 'luke004031.wav', 'luke004032.wav', 'luke004033.wav', 'luke004034.wav', 'luke004035.wav', 'luke004036.wav', 'luke004037.wav', 'luke004038.wav', 'luke004039.wav', 'luke00404.wav', 'luke004040.wav', 'luke004041.wav', 'luke004042.wav', 'luke004043.wav', 'luke004044.wav', 'luke00405.wav', 'luke00406.wav', 'luke00407.wav', 'luke00408.wav', 'luke00409.wav', 'luke00501.wav', 'luke005010.wav', 'luke005011.wav', 'luke005012.wav', 'luke005013.wav', 'luke005014.wav', 'luke005015.wav', 'luke005016.wav', 'luke005017.wav', 'luke005018.wav', 'luke005019.wav', 'luke00502.wav', 'luke005020.wav', 'luke005021.wav', 'luke005022.wav', 'luke005023.wav', 'luke005024.wav', 'luke005025.wav', 'luke005026.wav', 'luke005027.wav', 'luke005028.wav', 'luke005029.wav', 'luke00503.wav', 'luke005030.wav', 'luke005031.wav', 'luke005032.wav', 'luke005033.wav', 'luke005034.wav', 'luke005035.wav', 'luke005036.wav', 'luke005037.wav', 'luke005038.wav', 'luke005039.wav', 'luke00504.wav', 'luke00505.wav', 'luke00506.wav', 'luke00507.wav', 'luke00508.wav', 'luke00509.wav', 'luke00601.wav', 'luke006010.wav', 'luke006011.wav', 'luke006012.wav', 'luke006013.wav', 'luke006014.wav', 'luke006015.wav', 'luke006016.wav', 'luke006017.wav', 'luke006018.wav', 'luke006019.wav', 'luke00602.wav', 'luke006020.wav', 'luke006021.wav', 'luke006022.wav', 'luke006023.wav', 'luke006024.wav', 'luke006025.wav', 'luke006026.wav', 'luke006027.wav', 'luke006028.wav', 'luke006029.wav', 'luke00603.wav', 'luke006030.wav', 'luke006031.wav', 'luke006032.wav', 'luke006033.wav', 'luke006034.wav', 'luke006035.wav', 'luke006036.wav', 'luke006037.wav', 'luke006038.wav', 'luke006039.wav', 'luke00604.wav', 'luke006040.wav', 'luke006041.wav', 'luke006042.wav', 'luke006043.wav', 'luke006044.wav', 'luke006045.wav', 'luke006046.wav', 'luke006047.wav', 'luke006048.wav', 'luke006049.wav', 'luke00605.wav', 'luke00606.wav', 'luke00607.wav', 'luke00608.wav', 'luke00609.wav', 'luke00701.wav', 'luke007010.wav', 'luke007011.wav', 'luke007012.wav', 'luke007013.wav', 'luke007014.wav', 'luke007015.wav', 'luke007016.wav', 'luke007017.wav', 'luke007018.wav', 'luke007019.wav', 'luke00702.wav', 'luke007020.wav', 'luke007021.wav', 'luke007022.wav', 'luke007023.wav', 'luke007024.wav', 'luke007025.wav', 'luke007026.wav', 'luke007027.wav', 'luke007028.wav', 'luke007029.wav', 'luke00703.wav', 'luke007030.wav', 'luke007031.wav', 'luke007032.wav', 'luke007033.wav', 'luke007034.wav', 'luke007035.wav', 'luke007036.wav', 'luke007037.wav', 'luke007038.wav', 'luke007039.wav', 'luke00704.wav', 'luke007040.wav', 'luke007041.wav', 'luke007042.wav', 'luke007043.wav', 'luke007044.wav', 'luke007045.wav', 'luke007046.wav', 'luke007047.wav', 'luke007048.wav', 'luke007049.wav', 'luke00705.wav', 'luke007050.wav', 'luke00706.wav', 'luke00707.wav', 'luke00708.wav', 'luke00709.wav', 'luke00801.wav', 'luke008010.wav', 'luke008011.wav', 'luke008012.wav', 'luke008013.wav', 'luke008014.wav', 'luke008015.wav', 'luke008016.wav', 'luke008017.wav', 'luke008018.wav', 'luke008019.wav', 'luke00802.wav', 'luke008020.wav', 'luke008021.wav', 'luke008022.wav', 'luke008023.wav', 'luke008024.wav', 'luke008025.wav', 'luke008026.wav', 'luke008027.wav', 'luke008028.wav', 'luke008029.wav', 'luke00803.wav', 'luke008030.wav', 'luke008031.wav', 'luke008032.wav', 'luke008033.wav', 'luke008034.wav', 'luke008035.wav', 'luke008036.wav', 'luke008037.wav', 'luke008038.wav', 'luke008039.wav', 'luke00804.wav', 'luke008040.wav', 'luke008041.wav', 'luke008042.wav', 'luke008043.wav', 'luke008044.wav', 'luke008045.wav', 'luke008046.wav', 'luke008047.wav', 'luke008048.wav', 'luke008049.wav', 'luke00805.wav', 'luke008050.wav', 'luke008051.wav', 'luke008052.wav', 'luke008053.wav', 'luke008054.wav', 'luke008055.wav', 'luke008056.wav', 'luke00806.wav', 'luke00807.wav', 'luke00808.wav', 'luke00809.wav', 'luke00901.wav', 'luke009010.wav', 'luke009011.wav', 'luke009012.wav', 'luke009013.wav', 'luke009014.wav', 'luke009015.wav', 'luke009016.wav', 'luke009017.wav', 'luke009018.wav', 'luke009019.wav', 'luke00902.wav', 'luke009020.wav', 'luke009021.wav', 'luke009022.wav', 'luke009023.wav', 'luke009024.wav', 'luke009025.wav', 'luke009026.wav', 'luke009027.wav', 'luke009028.wav', 'luke009029.wav', 'luke00903.wav', 'luke009030.wav', 'luke009031.wav', 'luke009032.wav', 'luke009033.wav', 'luke009034.wav', 'luke009035.wav', 'luke009036.wav', 'luke009037.wav', 'luke009038.wav', 'luke009039.wav', 'luke00904.wav', 'luke009040.wav', 'luke009041.wav', 'luke009042.wav', 'luke009043.wav', 'luke009044.wav', 'luke009045.wav', 'luke009046.wav', 'luke009047.wav', 'luke009048.wav', 'luke009049.wav', 'luke00905.wav', 'luke009050.wav', 'luke009051.wav', 'luke009052.wav', 'luke009053.wav', 'luke009054.wav', 'luke009055.wav', 'luke009056.wav', 'luke009057.wav', 'luke009058.wav', 'luke009059.wav', 'luke00906.wav', 'luke009060.wav', 'luke009061.wav', 'luke00907.wav', 'luke00908.wav', 'luke00909.wav']\n"
     ]
    }
   ],
   "source": [
    "lukepath= \"D:\\Projects\\Internship NLP\\Wycliff-Mala Malasar\\Dataset_Luke\\Audio Files\"\n",
    "lukenames = os.listdir(lukepath)\n",
    "print(lukenames)\n",
    "\n",
    "luketsv = pd.read_csv(r\"D:\\Projects\\Internship NLP\\Wycliff-Mala Malasar\\Dataset_Luke\\transcript\\line_index_audios.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lukenewvers=[]\n",
    "lukenewsen =[]\n",
    "for i in range(0,50):\n",
    "    lukenewvers.append(str(lukepath)+'\\\\'+str(luketsv['audio_path'][i])+'.wav')\n",
    "    #lukenewvers.append(luketsv['path'][i])\n",
    "    lukenewsen.append(luketsv['label'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b5a538261e46e58f595cdf54be3daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, Audio, Value, Features\n",
    "\n",
    "dataset_dict = {\n",
    "    \"audio_path\": lukenewvers,\n",
    "    \"sentence\": lukenewsen\n",
    "}\n",
    "\n",
    "features = Features({\n",
    "    \"audio_path\": Audio(),\n",
    "    \"sentence\": Value(\"string\")\n",
    "})\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict).cast(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319a1720f90d48808ba311192608eda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b178059b58437bb92b9011cdd430f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22359718dcf49e59dde33d449c3d6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7a9bd206e34051a3769ead31ca58c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"basilkr/tamil_features_50audios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ab29b768ff4f43a6d13c294b706edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/415 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration basilkr--tamil_features_50audios-e833ce6655dfffb6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to C:/Users/basil/.cache/huggingface/datasets/basilkr___parquet/basilkr--tamil_features_50audios-e833ce6655dfffb6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979dca7431f64d6ea5b069a33942877d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7ad55866c94f0ab22d607821d354eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db0e1a17fda45fbaa52fcc869f0a4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932af3df973148dbbeaf2eb20ff966aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/basil/.cache/huggingface/datasets/basilkr___parquet/basilkr--tamil_features_50audios-e833ce6655dfffb6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447fcd27e7374814abaeede05394d294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset=DatasetDict()\n",
    "\n",
    "dataset = load_dataset(\"basilkr/tamil_features_50audios\",use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_path': {'path': 'luke00101.wav',\n",
       "  'array': array([ 0.00000000e+00,  0.00000000e+00,  1.52587891e-05, ...,\n",
       "         -1.52587891e-05, -3.05175781e-05,  3.05175781e-05]),\n",
       "  'sampling_rate': 44100},\n",
       " 'sentence': '. அதுக்கப்பறமு ஆண்டவரு இயேசு வேற எழுபத்திரெண்டு பேரவு தெரிஞ்சுகொண்டு, தான் போக இருந்த எல்லா ஊருகளுக்குமு, அவர்களவு ரெண்டு ரெண்டு பேரா போக சென்னரு. '}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column('audio_path',Audio(sampling_rate = 16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio_path', 'sentence'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\",  task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 . அதுக்கப்பறமு ஆண்டவரு இயேசு வேற எழுபத்திரெண்டு பேரவு தெரிஞ்சுகொண்டு, தான் போக இருந்த எல்லா ஊருகளுக்குமு, அவர்களவு ரெண்டு ரெண்டு பேரா போக சென்னரு. \n",
      "Decoded w/ special:    <|startoftranscript|><|transcribe|><|notimestamps|>. அதுக்கப்பறமு ஆண்டவரு இயேசு வேற எழுபத்திரெண்டு பேரவு தெரிஞ்சுகொண்டு, தான் போக இருந்த எல்லா ஊருகளுக்குமு, அவர்களவு ரெண்டு ரெண்டு பேரா போக சென்னரு. <|endoftext|>\n",
      "Decoded w/out special: . அதுக்கப்பறமு ஆண்டவரு இயேசு வேற எழுபத்திரெண்டு பேரவு தெரிஞ்சுகொண்டு, தான் போக இருந்த எல்லா ஊருகளுக்குமு, அவர்களவு ரெண்டு ரெண்டு பேரா போக சென்னரு. \n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = dataset[\"train\"][0][\"sentence\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\",  task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(batch):\n",
    "#     # load and resample audio data from 48 to 16kHz\n",
    "#     audio = batch[\"audio_path\"]\n",
    "\n",
    "#     batch[\"input_length\"] = len(batch[\"audio_path\"])\n",
    "#     from transformers import WhisperFeatureExtractor\n",
    "#     feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "\n",
    "#     # compute log-Mel input features from input audio array \n",
    "#     batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "#     from transformers import WhisperTokenizer\n",
    "\n",
    "#     tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", task=\"transcribe\")\n",
    "\n",
    "#     # encode target text to label ids \n",
    "#     batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "\n",
    "#         # compute labels length **with** special tokens! -> total label length\n",
    "#     batch[\"labels_length\"] = len(batch[\"labels\"])\n",
    "#     return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(batch):\n",
    "#     # load and resample audio data from 48 to 16kHz\n",
    "#     audio = batch[\"audio\"]\n",
    "\n",
    "#     # compute input length\n",
    "#     batch[\"input_length\"] = len(batch[\"audio\"])\n",
    "\n",
    "#     # compute log-Mel input features from input audio array \n",
    "#     batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "#     # encode target text to label ids \n",
    "#     batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "\n",
    "#     # compute labels length **with** special tokens! -> total label length\n",
    "#     batch[\"labels_length\"] = len(batch[\"labels\"])\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio_path\"]\n",
    "\n",
    "    # compute input length\n",
    "    batch[\"input_length\"] = len(batch[\"audio_path\"])\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "\n",
    "    # compute labels length\n",
    "    batch[\"labels_length\"] = len(tokenizer(batch[\"sentence\"], add_special_tokens=False).input_ids)\n",
    "    return batch\n",
    "\n",
    "MAX_DURATION_IN_SECONDS = 30.0\n",
    "max_input_length = MAX_DURATION_IN_SECONDS * 16000\n",
    "\n",
    "def filter_inputs(input_length):\n",
    "    if 0<input_length['input_length']<max_input_length:\n",
    "        return input_length\n",
    "    \n",
    "    # \"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
    "    # return 0 < input_length < max_input_length\n",
    "\n",
    "def filter_labels(labels_length):\n",
    "    if 0 < labels_length['labels_length']:\n",
    "        return labels_length\n",
    "\n",
    "    # \"\"\"Filter empty label sequences\"\"\"\n",
    "    # return 0 < len(labels_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cee9d35928e451494ef06c5eaf4184c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns= dataset.column_names[\"train\"])\n",
    "\n",
    "#dataset = dataset.map(filter_inputs)\n",
    "\n",
    "#dataset = dataset.filter(filter_labels, input_columns=[\"labels_length\"], remove_columns=dataset.column_names[\"labels_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bd579edf384147b4810f5078e1c13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(filter_labels,remove_columns=[\"labels_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset['train']['input_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126,\n",
       " 238,\n",
       " 87,\n",
       " 132,\n",
       " 121,\n",
       " 144,\n",
       " 85,\n",
       " 53,\n",
       " 57,\n",
       " 190,\n",
       " 290,\n",
       " 240,\n",
       " 378,\n",
       " 148,\n",
       " 126,\n",
       " 224,\n",
       " 228,\n",
       " 72,\n",
       " 222,\n",
       " 185,\n",
       " 375,\n",
       " 368,\n",
       " 126,\n",
       " 202,\n",
       " 241,\n",
       " 124,\n",
       " 248,\n",
       " 122,\n",
       " 109,\n",
       " 286,\n",
       " 104,\n",
       " 170,\n",
       " 133,\n",
       " 170,\n",
       " 196,\n",
       " 205,\n",
       " 184,\n",
       " 149,\n",
       " 124,\n",
       " 284,\n",
       " 83,\n",
       " 212,\n",
       " 298,\n",
       " 181,\n",
       " 52,\n",
       " 231,\n",
       " 162,\n",
       " 170,\n",
       " 191,\n",
       " 177,\n",
       " 128,\n",
       " 103,\n",
       " 116,\n",
       " 96,\n",
       " 271,\n",
       " 207,\n",
       " 120,\n",
       " 167,\n",
       " 206,\n",
       " 124,\n",
       " 188,\n",
       " 161,\n",
       " 122,\n",
       " 223,\n",
       " 157,\n",
       " 297,\n",
       " 159,\n",
       " 251,\n",
       " 167,\n",
       " 138,\n",
       " 365,\n",
       " 374,\n",
       " 332,\n",
       " 262,\n",
       " 267,\n",
       " 463,\n",
       " 172,\n",
       " 257,\n",
       " 123,\n",
       " 133,\n",
       " 148,\n",
       " 130,\n",
       " 205,\n",
       " 450,\n",
       " 273,\n",
       " 355,\n",
       " 149,\n",
       " 283,\n",
       " 184,\n",
       " 177]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset['train']['labels_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b6bde798b34a059af6f772fb86c09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a variable of type <class 'bool'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\Internship NLP\\Wycliff-Mala Malasar\\Code\\Final_Dataset_Creation_WhisperModel.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(filter_inputs, input_columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_length\u001b[39;49m\u001b[39m\"\u001b[39;49m], remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_length\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mfilter(filter_labels, input_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mlabels_length\u001b[39m\u001b[39m\"\u001b[39m], remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mlabels_length\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\dataset_dict.py:816\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 816\u001b[0m     {\n\u001b[0;32m    817\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    818\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    822\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    830\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    834\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\dataset_dict.py:817\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    816\u001b[0m     {\n\u001b[1;32m--> 817\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    818\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    822\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    830\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    834\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py:2815\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2812\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   2814\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 2815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   2816\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   2817\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   2818\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   2819\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   2820\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   2821\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2822\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   2823\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   2824\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   2825\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   2826\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   2827\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   2828\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2829\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   2830\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   2831\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   2832\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   2833\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   2834\u001b[0m     )\n\u001b[0;32m   2835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2837\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py:546\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    547\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    548\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    549\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py:513\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    507\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    508\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    509\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    510\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    511\u001b[0m }\n\u001b[0;32m    512\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    514\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    515\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py:3221\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   3219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[0;32m   3220\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m-> 3221\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[0;32m   3222\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[0;32m   3223\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py:3123\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3121\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n\u001b[1;32m-> 3123\u001b[0m     validate_function_output(processed_inputs, indices)\n\u001b[0;32m   3124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m update_data:\n\u001b[0;32m   3125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# Nothing to update, let's move on\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py:3068\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.validate_function_output\u001b[1;34m(processed_inputs, indices)\u001b[0m\n\u001b[0;32m   3066\u001b[0m \u001b[39m\"\"\"Validate output of the map function.\"\"\"\u001b[39;00m\n\u001b[0;32m   3067\u001b[0m \u001b[39mif\u001b[39;00m processed_inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable)):\n\u001b[1;32m-> 3068\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   3069\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `function` which is applied to all elements of table returns a variable of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(processed_inputs)\u001b[39m}\u001b[39;00m\u001b[39m. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3070\u001b[0m     )\n\u001b[0;32m   3071\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(indices, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, Mapping):\n\u001b[0;32m   3072\u001b[0m     allowed_batch_return_types \u001b[39m=\u001b[39m (\u001b[39mlist\u001b[39m, np\u001b[39m.\u001b[39mndarray, pd\u001b[39m.\u001b[39mSeries)\n",
      "\u001b[1;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a variable of type <class 'bool'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects."
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(filter_inputs, input_columns=[\"input_length\"], remove_columns=[\"input_length\"])\n",
    "\n",
    "dataset = dataset.filter(filter_labels, input_columns=[\"labels_length\"], remove_columns=[\"labels_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DURATION_IN_SECONDS = 30.0\n",
    "max_input_length = MAX_DURATION_IN_SECONDS * 16000\n",
    "\n",
    "def filter_inputs(input_length):\n",
    "    \"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
    "    return 0 < input_length < max_input_length\n",
    "\n",
    "def filter_labels(labels_length):\n",
    "    \"\"\"Filter empty label sequences\"\"\"\n",
    "    return 0 < len(labels_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DatasetDict.filter() got an unexpected keyword argument 'remove_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\Internship NLP\\Wycliff-Mala Malasar\\Code\\Final_Dataset_Creation_WhisperModel.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mfilter(filter_inputs, input_columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_length\u001b[39;49m\u001b[39m\"\u001b[39;49m], remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_length\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "\u001b[1;31mTypeError\u001b[0m: DatasetDict.filter() got an unexpected keyword argument 'remove_columns'"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(filter_inputs, input_columns=[\"input_length\"], remove_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'audio_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 546, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 513, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\fingerprint.py\", line 480, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3221, in _map_single\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\n  File \"C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py\", line 3112, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\basil\\AppData\\Local\\Temp\\ipykernel_14904\\1625869777.py\", line 3, in prepare_dataset\n  File \"C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\formatting\\formatting.py\", line 280, in __getitem__\n    value = self.data[key]\nKeyError: 'audio_path'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\Internship NLP\\Wycliff-Mala Malasar\\Code\\Final_Dataset_Creation_WhisperModel.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(prepare_dataset, remove_columns\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mcolumn_names[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], num_proc\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\dataset_dict.py:816\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 816\u001b[0m     {\n\u001b[0;32m    817\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    818\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    822\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    830\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    834\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\dataset_dict.py:817\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    816\u001b[0m     {\n\u001b[1;32m--> 817\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    818\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    822\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    830\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    834\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\datasets\\arrow_dataset.py:2928\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2923\u001b[0m         \u001b[39massert\u001b[39;00m (\n\u001b[0;32m   2924\u001b[0m             \u001b[39mlen\u001b[39m(results) \u001b[39m==\u001b[39m nb_of_missing_shards\n\u001b[0;32m   2925\u001b[0m         ), \u001b[39m\"\u001b[39m\u001b[39mThe number of missing cached shards needs to correspond to the number of `_map_single` we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre running\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2927\u001b[0m         \u001b[39mfor\u001b[39;00m index, async_result \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mitems():\n\u001b[1;32m-> 2928\u001b[0m             transformed_shards[index] \u001b[39m=\u001b[39m async_result\u001b[39m.\u001b[39;49mget()\n\u001b[0;32m   2930\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[0;32m   2931\u001b[0m     transformed_shards\u001b[39m.\u001b[39mcount(\u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   2932\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mAll shards have to be defined Datasets, none should still be missing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2934\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConcatenating \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m shards\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[1;31mKeyError\u001b[0m: 'audio_path'"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.max_length = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=40,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_NO_GIT\"] = \"1\"\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 50\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 40\n",
      "  Number of trainable parameters = 241734912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778b952b81974c35bf9fb115c75e3307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f458eac46548e8a0df3bc600b97bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./whisper-small-hi\\checkpoint-10\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-10\\config.json\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-10\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4583170413970947, 'eval_wer': 101.23203285420945, 'eval_runtime': 754.3607, 'eval_samples_per_second': 0.066, 'eval_steps_per_second': 0.009, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./whisper-small-hi\\checkpoint-10\\pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-hi\\checkpoint-10\\preprocessor_config.json\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79a97b7273b4bfc8437e25924df6af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./whisper-small-hi\\checkpoint-20\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-20\\config.json\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-20\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9955803155899048, 'eval_wer': 98.04928131416838, 'eval_runtime': 691.7303, 'eval_samples_per_second': 0.072, 'eval_steps_per_second': 0.01, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./whisper-small-hi\\checkpoint-20\\pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-hi\\checkpoint-20\\preprocessor_config.json\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3242, 'learning_rate': 5.000000000000001e-07, 'epoch': 6.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dfc57e7a3e49998a4468ef582a406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./whisper-small-hi\\checkpoint-30\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-30\\config.json\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-30\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5232313871383667, 'eval_wer': 96.50924024640656, 'eval_runtime': 681.6751, 'eval_samples_per_second': 0.073, 'eval_steps_per_second': 0.01, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./whisper-small-hi\\checkpoint-30\\pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-hi\\checkpoint-30\\preprocessor_config.json\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3a391a0d8e44ada6a9289069a8c95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./whisper-small-hi\\checkpoint-40\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-40\\config.json\n",
      "Configuration saved in ./whisper-small-hi\\checkpoint-40\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1846407651901245, 'eval_wer': 100.2053388090349, 'eval_runtime': 686.7788, 'eval_samples_per_second': 0.073, 'eval_steps_per_second': 0.01, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./whisper-small-hi\\checkpoint-40\\pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-hi\\checkpoint-40\\preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./whisper-small-hi\\checkpoint-30 (score: 96.50924024640656).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24612.7237, 'train_samples_per_second': 0.026, 'train_steps_per_second': 0.002, 'train_loss': 1.989386796951294, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=1.989386796951294, metrics={'train_runtime': 24612.7237, 'train_samples_per_second': 0.026, 'train_steps_per_second': 0.002, 'train_loss': 1.989386796951294, 'epoch': 10.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/basilkr/whisper-small-hi into local empty directory.\n",
      "Saving model checkpoint to ./whisper-small-hi\n",
      "Configuration saved in ./whisper-small-hi\\config.json\n",
      "Configuration saved in ./whisper-small-hi\\generation_config.json\n",
      "Model weights saved in ./whisper-small-hi\\pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-hi\\preprocessor_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788351efe4714c41bffec7a071e11586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/922M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eac57bd0704f068e0f7e02e7278075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/basilkr/whisper-small-hi\n",
      "   ddecc65..8564ce4  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Automatic Speech Recognition', 'type': 'automatic-speech-recognition'}, 'metrics': [{'name': 'Wer', 'type': 'wer', 'value': 100.2053388090349}]}\n",
      "To https://huggingface.co/basilkr/whisper-small-hi\n",
      "   8564ce4..42983ab  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/basilkr/whisper-small-hi/commit/8564ce4acb40e5b8ffc3c7194a67b9e6116b56a6'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"whisper_tamil_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in C:\\Users\\basil\\AppData\\Local\\Temp\\tmp3fh95pi0\\tokenizer_config.json\n",
      "Special tokens file saved in C:\\Users\\basil\\AppData\\Local\\Temp\\tmp3fh95pi0\\special_tokens_map.json\n",
      "added tokens file saved in C:\\Users\\basil\\AppData\\Local\\Temp\\tmp3fh95pi0\\added_tokens.json\n",
      "Uploading the following files to basilkr/whisper_tamil_50: added_tokens.json,merges.txt,normalizer.json,special_tokens_map.json,tokenizer_config.json,vocab.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/basilkr/whisper_tamil_50/commit/38a76389e20149fbadbeda8c300d06825c5e100a', commit_message='Upload tokenizer', commit_description='', oid='38a76389e20149fbadbeda8c300d06825c5e100a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"whisper_tamil_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nPushToHubCallback requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\Internship NLP\\Wycliff-Mala Malasar\\Code\\Final_Dataset_Creation_WhisperModel.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m PushToHubCallback\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m push_to_hub_callback \u001b[39m=\u001b[39m PushToHubCallback(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mProjects\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mInternship NLP\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mWycliff-Mala Malasar\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mCode\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mwhisper-small-hi\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcheckpoint-30\u001b[39;49m\u001b[39m\"\u001b[39;49m, tokenizer\u001b[39m=\u001b[39;49mtokenizer, hub_model_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbasilkr/whisper_tamil_50\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/Internship%20NLP/Wycliff-Mala%20Malasar/Code/Final_Dataset_Creation_WhisperModel.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\dummy_tf_objects.py:126\u001b[0m, in \u001b[0;36mPushToHubCallback.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 126\u001b[0m     requires_backends(\u001b[39mself\u001b[39;49m, [\u001b[39m\"\u001b[39;49m\u001b[39mtf\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\import_utils.py:1033\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1033\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[39m.\u001b[39mformat(name))\n\u001b[0;32m   1035\u001b[0m checks \u001b[39m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends)\n\u001b[0;32m   1036\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n",
      "\u001b[1;31mImportError\u001b[0m: \nPushToHubCallback requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"D:\\Projects\\Internship NLP\\Wycliff-Mala Malasar\\Code\\whisper-small-hi\\checkpoint-30\", tokenizer=tokenizer, hub_model_id=\"basilkr/whisper_tamil_50\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
